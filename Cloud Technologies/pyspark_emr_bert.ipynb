{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52a64306-c721-45c3-b27c-170f809987dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b53ed606-9f26-42ac-8373-4e88a1f52d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "findspark initialized: /usr/lib/spark\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "\n",
    "# 初始化 findspark 并指向 EMR 的 Spark 安装路径\n",
    "findspark.init('/usr/lib/spark')\n",
    "\n",
    "# 确认是否正确初始化\n",
    "print(\"findspark initialized:\", findspark.find())\n",
    "\n",
    "import findspark\n",
    "findspark.init(\"/usr/lib/spark\")  # 替换为 Spark 的实际路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "140846aa-9893-4962-92e5-2c79721149bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 配置环境变量\n",
    "os.environ[\"SPARK_HOME\"] = \"/usr/lib/spark\"\n",
    "os.environ[\"HADOOP_CONF_DIR\"] = \"/etc/hadoop/conf\"\n",
    "os.environ[\"YARN_CONF_DIR\"] = \"/etc/hadoop/conf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d75289e3-48e6-415d-a17e-15ed1d9db674",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/11/23 17:34:37 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "24/11/23 17:34:46 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Attempted to request executors before the AM has registered!\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"EMR CSV Export\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "710e3d21-11f4-4406-b278-5d3e6a8f06e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n",
      "24/11/23 17:34:47 WARN ConfigurationHelper: Option fs.s3a.connection.establish.timeout is too low (5,000 ms). Setting to 15,000 ms instead\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 读取数据\n",
    "s3_file_path = \"s3a://dcu-dmv-bucket/Bitcoin_tweets.csv\"\n",
    "\n",
    "# 读取 S3 数据\n",
    "df = spark.read.csv(s3_file_path, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1fa412d3-3fac-4ea6-9a8a-e9c35af290ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.drop(['user_name','user_location','user_description','user_created','user_friends','user_favourites','user_verified','source','is_retweet'],axis=1,inplace=True)\n",
    "# 假设原始 PySpark DataFrame 名为 df\n",
    "\n",
    "columns_to_drop = ['user_name', 'user_location', 'user_description', \n",
    "                   'user_created', 'user_friends', 'user_favourites', \n",
    "                   'user_verified', 'source', 'is_retweet']\n",
    "\n",
    "# 删除指定列\n",
    "df = df.drop(*columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afb3b964-7bf6-44de-9d7b-cb67f2ac2bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lower, col\n",
    "df = df.withColumn(\"hashtags\", lower(col(\"hashtags\")))  # 转换为小写"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "abce2099-fbcb-4385-a648-b0aaac05fbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 筛选包含特定关键字的行\n",
    "df_filtered = df.filter(\n",
    "    col(\"hashtags\").rlike(r\"\\b(btc|bitcoin|bitcoins)\\b\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12dc216b-39d4-4057-98a7-d97959263c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:=================================================>       (14 + 2) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (1400452, 4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# df.shape\n",
    "def get_shape(spark_df):\n",
    "    return (spark_df.count(), len(spark_df.columns))\n",
    "\n",
    "shape = get_shape(df_filtered)\n",
    "print(f\"Shape: {shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c5064e6-d142-4523-9ff2-1cff1bf5fffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df_filtered.drop('hashtags')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9dd6f910-1e14-475b-b330-819c43fb3b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_format, to_timestamp\n",
    "\n",
    "# 将日期字符串转换为新的解析器兼容的格式\n",
    "df_filtered = df_filtered.withColumn(\"date\", to_timestamp(\"date\", \"yyyy-MM-dd HH:mm:ss\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d4c8c920-f586-4730-8abb-cfaea89fc792",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df_filtered.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d58f4b0e-6ca7-4d72-8b61-1cc7c908797f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:==========================================>               (8 + 3) / 11]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------------+--------------------+\n",
      "|user_followers|               date|                text|\n",
      "+--------------+-------------------+--------------------+\n",
      "|         511.0|2022-01-14 23:59:59|Death Cross #bitc...|\n",
      "|          26.0|2022-01-14 23:59:56|#bitcoin is revol...|\n",
      "|          94.0|2022-01-14 23:59:56|Teaser! #Bitcoin ...|\n",
      "|          71.0|2022-01-14 23:59:43|#Bitcoin $BTC Nic...|\n",
      "|         152.0|2022-01-14 23:59:42|👋 Hey! Wait! hel...|\n",
      "|           0.0|2022-01-14 23:59:20|#Bitcoin is a vol...|\n",
      "|         206.0|2022-01-14 23:59:14|4 hour top movers...|\n",
      "|           6.0|2022-01-14 23:58:41|Bullishnewfie fou...|\n",
      "|           0.0|2022-01-14 23:58:10|u rn on way to bu...|\n",
      "|         477.0|2022-01-14 23:57:50|Current #Bitcoin ...|\n",
      "|           7.0|2022-01-14 23:57:46|7% inflation? Bal...|\n",
      "|          50.0|2022-01-14 23:57:31|@DoombergT #bitco...|\n",
      "|       76481.0|2022-01-14 23:57:07|#linkedin #twitte...|\n",
      "|         230.0|2022-01-14 23:57:05|👋 A new block wa...|\n",
      "|         178.0|2022-01-14 23:56:31|Traders say #Bitc...|\n",
      "|         373.0|2022-01-14 23:56:25|\"\"\"Bitcoin will r...|\n",
      "|         241.0|2022-01-14 23:56:05|BPOD found #bitco...|\n",
      "|        1120.0|2022-01-14 23:55:46|[857] #010 #Bitco...|\n",
      "|          73.0|2022-01-14 23:55:33|O'Melveny, DLA Pi...|\n",
      "|        1821.0|2022-01-14 23:55:31|THIS VIDEO WILL W...|\n",
      "+--------------+-------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "# 定义开始日期和结束日期\n",
    "start_date = '2022-01-14'\n",
    "end_date = '2022-04-14'\n",
    "# 按条件筛选行\n",
    "df_filtered = df_filtered.filter((col(\"date\") > start_date) & (col(\"date\") <= end_date))\n",
    "\n",
    "# 显示筛选后的结果\n",
    "df_filtered.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d6617b1-e0de-43c2-80b3-f40933a84b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 筛选 user_followers > 10 的行\n",
    "df_filtered = df_filtered.filter(col(\"user_followers\") > 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ea7a5b1-fb72-40d0-a920-6e283a00505b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:=================================================>       (14 + 2) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (152763, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "shape = get_shape(df_filtered)\n",
    "print(f\"Shape: {shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e313386-c215-417a-ae40-feb51a68cf4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "647f702f-4e47-4ec0-9140-beb4aa8fdedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lower, regexp_replace, split, size\n",
    "df_filtered = df_filtered.withColumn(\"text\", lower(col(\"text\")))  # 转换为小写\n",
    "df_filtered = df_filtered.withColumn(\"text\", regexp_replace(col(\"text\"), \"@[A-Za-z0-9_]+\", \"\"))  # 删除用户名\n",
    "df_filtered = df_filtered.withColumn(\"text\", regexp_replace(col(\"text\"), \"#[A-Za-z0-9_]+\", \"\"))  # 删除话题标签\n",
    "df_filtered = df_filtered.withColumn(\"text\", regexp_replace(col(\"text\"), r\"http\\S+\", \"\"))  # 删除 URL\n",
    "df_filtered = df_filtered.withColumn(\"text\", regexp_replace(col(\"text\"), r\"www.\\S+\", \"\"))  # 删除以 www. 开头的 URL\n",
    "df_filtered = df_filtered.withColumn(\"text\", regexp_replace(col(\"text\"), r\"[()!?]\", \" \"))  # 替换标点符号\n",
    "df_filtered = df_filtered.withColumn(\"text\", regexp_replace(col(\"text\"), r\"\\[.*?\\]\", \" \"))  # 删除方括号内容\n",
    "df_filtered = df_filtered.withColumn(\"text\", regexp_replace(col(\"text\"), r\"[^a-z0-9]\", \" \"))  # 替换非字母数字字符为空格"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "70ed6925-67e3-4ec0-ab10-5b27c6c3065f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 13:===============================>                         (6 + 5) / 11]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+\n",
      "|               date|                text|\n",
      "+-------------------+--------------------+\n",
      "|2022-01-14 23:59:59| death cross  dump  |\n",
      "|2022-01-14 23:59:56| is revolutionizi...|\n",
      "|2022-01-14 23:59:56|         teaser     |\n",
      "|2022-01-14 23:59:43|  btc nice bounce...|\n",
      "|2022-01-14 23:59:42|  hey  wait  help...|\n",
      "|2022-01-14 23:59:14|4 hour top movers...|\n",
      "|2022-01-14 23:57:50|current  price is...|\n",
      "|2022-01-14 23:57:31|      fixes this    |\n",
      "|2022-01-14 23:57:07|                 ...|\n",
      "|2022-01-14 23:57:05|  a new block was...|\n",
      "|2022-01-14 23:56:31|traders say  run ...|\n",
      "|2022-01-14 23:56:25|   bitcoin will r...|\n",
      "|2022-01-14 23:56:05|bpod found  in a ...|\n",
      "|2022-01-14 23:55:46|       is really ...|\n",
      "|2022-01-14 23:55:33|o melveny  dla pi...|\n",
      "|2022-01-14 23:55:31|this video will w...|\n",
      "|2022-01-14 23:55:26|          fixed this|\n",
      "|2022-01-14 23:55:17|this video will w...|\n",
      "|2022-01-14 23:55:09|this video will w...|\n",
      "|2022-01-14 23:54:52|about 70  of  in ...|\n",
      "+-------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 筛选 text 列中单词数大于 3 的行\n",
    "df2 = df_filtered.filter(size(split(col(\"text\"), \" \")) > 3)\n",
    "df2 = df2.drop('user_followers')\n",
    "# 显示结果\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3136fe82-40bc-4b5f-8232-e7858fdd90fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 14:====================================================>   (15 + 1) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (152086, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "shape = get_shape(df2)\n",
    "print(f\"Shape: {shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "24ae3256-b6fc-4293-842a-02235b1db82a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib64/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "caeafd84-ee6d-4800-9f58-dcefd96613ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "786cd9bd-6a71-43f5-b63d-b999e0a2a42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sentiment_score(text):\n",
    "#     tokens = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "#     result = model(tokens)\n",
    "#     return int(torch.argmax(result.logits)) - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8cc9a372-ada5-41d2-82d2-f46fbbebe9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = 'bitcoin just ok'\n",
    "# sentiment_score(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9920a555-c2b7-4759-ba3f-baaad08428d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas_df = df2.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "96fa2ca2-cf60-48a5-801d-f694c2538d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4c4e34f1-3344-435d-8b23-9544fa11658a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f7f43aa9-5aa3-4d82-b602-ad5256e3703a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 19:==================================================>     (10 + 1) / 11]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+---------+\n",
      "|               date|                text|sentiment|\n",
      "+-------------------+--------------------+---------+\n",
      "|2022-01-14 23:59:59| death cross  dump  |       -2|\n",
      "|2022-01-14 23:59:56| is revolutionizi...|        2|\n",
      "|2022-01-14 23:59:56|         teaser     |        0|\n",
      "|2022-01-14 23:59:43|  btc nice bounce...|        0|\n",
      "|2022-01-14 23:59:42|  hey  wait  help...|        2|\n",
      "|2022-01-14 23:59:14|4 hour top movers...|        2|\n",
      "|2022-01-14 23:57:50|current  price is...|        2|\n",
      "|2022-01-14 23:57:31|      fixes this    |        2|\n",
      "|2022-01-14 23:57:07|                 ...|        2|\n",
      "|2022-01-14 23:57:05|  a new block was...|       -2|\n",
      "|2022-01-14 23:56:31|traders say  run ...|       -2|\n",
      "|2022-01-14 23:56:25|   bitcoin will r...|        2|\n",
      "|2022-01-14 23:56:05|bpod found  in a ...|        2|\n",
      "|2022-01-14 23:55:46|       is really ...|       -2|\n",
      "|2022-01-14 23:55:33|o melveny  dla pi...|        2|\n",
      "|2022-01-14 23:55:31|this video will w...|        2|\n",
      "|2022-01-14 23:55:26|          fixed this|        2|\n",
      "|2022-01-14 23:55:17|this video will w...|        2|\n",
      "|2022-01-14 23:55:09|this video will w...|        2|\n",
      "|2022-01-14 23:54:52|about 70  of  in ...|       -2|\n",
      "+-------------------+--------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "# 加载 BERT 模型和分词器\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "\n",
    "# 定义情感分析函数\n",
    "def sentiment_score(text):\n",
    "    tokens = tokenizer.encode(text[:512], return_tensors=\"pt\")\n",
    "    result = model(tokens)\n",
    "    return int(torch.argmax(result.logits)) - 2\n",
    "\n",
    "# 将函数注册为 UDF\n",
    "sentiment_udf = udf(sentiment_score, IntegerType())\n",
    "\n",
    "# 创建新的 Spark DataFrame 列，添加 sentiment 列\n",
    "df2 = df2.withColumn(\"sentiment\", sentiment_udf(df2[\"text\"]))\n",
    "\n",
    "# 显示结果\n",
    "df2.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b5d0cd9e-232f-4505-81e9-7172e0f3ff89",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df2.drop('text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "244e388c-064d-471e-8dc2-0420032caed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 22:==================================================>     (10 + 1) / 11]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------+\n",
      "|               date|sentiment|\n",
      "+-------------------+---------+\n",
      "|2022-01-14 23:59:59|       -2|\n",
      "|2022-01-14 23:59:56|        2|\n",
      "|2022-01-14 23:59:56|        0|\n",
      "|2022-01-14 23:59:43|        0|\n",
      "|2022-01-14 23:59:42|        2|\n",
      "|2022-01-14 23:59:14|        2|\n",
      "|2022-01-14 23:57:50|        2|\n",
      "|2022-01-14 23:57:31|        2|\n",
      "|2022-01-14 23:57:07|        2|\n",
      "|2022-01-14 23:57:05|       -2|\n",
      "|2022-01-14 23:56:31|       -2|\n",
      "|2022-01-14 23:56:25|        2|\n",
      "|2022-01-14 23:56:05|        2|\n",
      "|2022-01-14 23:55:46|       -2|\n",
      "|2022-01-14 23:55:33|        2|\n",
      "|2022-01-14 23:55:31|        2|\n",
      "|2022-01-14 23:55:26|        2|\n",
      "|2022-01-14 23:55:17|        2|\n",
      "|2022-01-14 23:55:09|        2|\n",
      "|2022-01-14 23:54:52|       -2|\n",
      "+-------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3c87ee4b-e9ce-49a8-b96e-0abe48e180e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pandas_df = df2.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7ddf7a-012e-49e3-9eae-8ad673da8665",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 23:==========================================>             (12 + 4) / 16]\r"
     ]
    }
   ],
   "source": [
    "# 将 DataFrame 写入 S3，使用 gzip 压缩\n",
    "df3.write.option(\"header\", \"true\") \\\n",
    "    .option(\"compression\", \"gzip\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .csv(\"s3a://dcu-dmv-bucket/no_text/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197a2fd3-8ccb-4b17-afdb-d4548db67411",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8a76d5-4688-470e-818e-fc345506e0cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda6bf3c-6f87-4ecf-a3fc-156e1219ce9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e754b05e-3edc-4acf-ac94-8e653fdef5af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea048c5-3ca2-4a00-b501-288a052d0ca7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20327ecf-59d7-4b7f-8cfd-9aef8771b51f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b4f8c3-8c22-459c-b7eb-25ab5fd0f34a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "48660499-56a5-469d-a30e-f5469905bd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 将 df2 写入到 S3 的指定路径\n",
    "# df2.write.csv(\"s3a://dcu-dmv-bucket/with_text\", \n",
    "#               header=True, mode=\"overwrite\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13476f74-e00a-402a-ba2e-b889319418b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc85b8db-cd47-4384-91ea-aca8626c6391",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8f45d3-30e2-4889-af32-c54f667ffe5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed26104-5d0f-4b13-ac1a-a33c11726237",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9217306-6785-45af-aa0c-fbb4d5f7b088",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985b535f-5a19-4510-8a53-8b77a145f68f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799bfe5a-3354-4233-97db-ce1e21cf0bf8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "39b03b6b-e991-4286-8253-cc240da7ca8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/11/23 17:38:24 WARN BlockManagerMasterEndpoint: No more replicas available for broadcast_21_python !\n"
     ]
    }
   ],
   "source": [
    "# pandas_df['sentiment'] = pandas_df['text'].apply(lambda x: sentiment_score(x[:512]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7096c9b9-c5e8-436a-bb84-e5e03f075621",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920c9de8-af22-48ca-ab14-0bf12b84de14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb01b1fb-08ff-4768-84b9-9c505ce25fcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8e9210-28f3-440d-8b86-98a6271de97b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674c622c-ae39-49b9-a71f-13c095aeb7a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
